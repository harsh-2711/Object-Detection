{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import struct, os\n",
    "import re, numpy as np\n",
    "from skimage import transform\n",
    "import itertools, operator\n",
    "import pickle\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Yolo v2 config\n",
    "# Load pre-trained weights from darknet 23 layer\n",
    "# Understand the Yolo v2 training process \n",
    "#            - Data Aug\n",
    "#            - Multi Scale Training\n",
    "#            - Loss Fn\n",
    "#            - Learning Rate\n",
    "#            - Optimizer setting\n",
    "# \n",
    "# Understand how the model requires <input, labels> feed dict\n",
    "# Read PASCAL data in the format needed\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "# Transforms classes\n",
    "# Random scale\n",
    "# Flip\n",
    "# x, y reposition\n",
    "class RandomCrop(object):\n",
    "    \n",
    "    def imcv2_affine_trans(self, im):\n",
    "        # Scale and translate\n",
    "        h, w, c = im.shape\n",
    "        scale = np.random.uniform() / 10. + 1.\n",
    "        max_offx = (scale-1.) * w\n",
    "        max_offy = (scale-1.) * h\n",
    "        offx = int(np.random.uniform() * max_offx)\n",
    "        offy = int(np.random.uniform() * max_offy)\n",
    "\n",
    "        im = cv2.resize(im, (0,0), fx = scale, fy = scale)\n",
    "        im = im[offy : (offy + h), offx : (offx + w)]\n",
    "\n",
    "        return im, [w, h], [scale, [offx, offy]]\n",
    "\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "        result = self.imcv2_affine_trans(image)\n",
    "        image, dims, trans_param = result\n",
    "        scale, offs = trans_param\n",
    "        \n",
    "        offs = np.array(offs*2)\n",
    "        dims = np.array(dims*2)\n",
    "        bboxes = deepcopy(bboxes)\n",
    "        bboxes[:, 1:] = np.array(bboxes[:, 1:]*scale - offs, np.int64)\n",
    "        bboxes[:, 1:] = np.maximum(np.minimum(bboxes[:, 1:], dims), 0)\n",
    "        \n",
    "        check_errors = (((bboxes[:, 1] >= bboxes[:, 3]) | (bboxes[:, 2] >= bboxes[:, 4])) & (bboxes[:, 0]!=-1))\n",
    "        if sum(check_errors) > 0:\n",
    "            bool_mask = ~ check_errors\n",
    "            bboxes = bboxes[bool_mask]\n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "\n",
    "        bboxes = deepcopy(bboxes)\n",
    "        flip = np.random.binomial(1, .5)\n",
    "        if flip: \n",
    "            w = image.shape[1]\n",
    "            image = cv2.flip(image, 1)\n",
    "            backup_min = deepcopy(bboxes[:, 1])\n",
    "            bboxes[:, 1] = w - bboxes[:, 3]\n",
    "            bboxes[:, 3] = w - backup_min\n",
    "        \n",
    "        if sum(((bboxes[:, 1] >= bboxes[:, 3]) | (bboxes[:, 2] >= bboxes[:, 4])) & (bboxes[:, 0]!=-1)) > 0:\n",
    "            print (\"random flip\")\n",
    "        \n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "class Rescale(object):\n",
    "    \n",
    "    def __init__(self, output):\n",
    "        self.new_h, self.new_w = output\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "        h, w, c = image.shape\n",
    "        new_h = int(self.new_h)\n",
    "        new_w = int(self.new_w)\n",
    "        image = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        bboxes = deepcopy(bboxes)\n",
    "        bboxes = np.array(bboxes, np.float64)\n",
    "        bboxes[:, 1] *= new_w*1.0/w\n",
    "        bboxes[:, 2] *= new_h*1.0/h\n",
    "        bboxes[:, 3] *= new_w*1.0/w\n",
    "        bboxes[:, 4] *= new_h*1.0/h\n",
    "        if sum(((bboxes[:, 1] >= bboxes[:, 3]) | (bboxes[:, 2] >= bboxes[:, 4])) & (bboxes[:, 0]!=-1)) > 0:\n",
    "            print (\"random scale\", bboxes, sample['bboxes'], new_w, new_h, w, h)\n",
    "\n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "class TransformBoxCoords(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "        height, width, _ = image.shape\n",
    "        \n",
    "        bboxes = deepcopy(bboxes)\n",
    "        bboxes = np.array(bboxes, np.float64)\n",
    "        x = 0.5 * (bboxes[:, 1] + bboxes[:, 3])\n",
    "        y = 0.5 * (bboxes[:, 2] + bboxes[:, 4])\n",
    "        w = 1. * (bboxes[:, 3] - bboxes[:, 1])\n",
    "        h = 1. * (bboxes[:, 4] - bboxes[:, 2])\n",
    "        if sum(((w <= 0) | (h <= 0) | (x <= 0) | (y <= 0)) & (bboxes[:, 0]!=-1))>0:\n",
    "            print (\"up\", bboxes, sample[\"bboxes\"])\n",
    "        bboxes[:, 1] = x/width\n",
    "        bboxes[:, 2] = y/height\n",
    "        bboxes[:, 3] = w/width\n",
    "        bboxes[:, 4] = h/height\n",
    "        if sum(((bboxes[:, 1] <0) | (bboxes[:, 2]<0) | (bboxes[:, 3]<=0) | (bboxes[:, 4]<=0)) & (bboxes[:, 0]!=-1)) > 0:\n",
    "            print (\"random transform box coords\")\n",
    "\n",
    "        \n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "class Normalize(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "        image = np.array(image, np.float64)\n",
    "        image /= 255.0\n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "class EliminateSmallBoxes(object):\n",
    "    \n",
    "    def __init__(self, thresh):\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "        bool_mask = ((bboxes[: , 3] > self.thresh) & (bboxes[: , 4] > self.thresh))\n",
    "        bboxes = bboxes[bool_mask]\n",
    "        return {\"image\": image, \"bboxes\": bboxes}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bboxes = sample['image'], sample['bboxes']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        if len(bboxes) == 0:\n",
    "            return {'image': torch.from_numpy(image), 'bboxes': torch.DoubleTensor()}\n",
    "        return {'image': torch.from_numpy(image), 'bboxes': torch.from_numpy(bboxes)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =  np.array(['sheep', 'horse', 'bicycle', 'bottle', 'cow', 'sofa', 'car', 'dog', 'cat', 'person', 'train', 'diningtable', 'aeroplane', 'bus', 'pottedplant', 'tvmonitor', 'chair', 'bird', 'boat', 'motorbike'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, folderpath, sample=-1, transform=None, files = [], max_truth=30):\n",
    "        \n",
    "        \"\"\"\n",
    "            Reading all image names without the extension.\n",
    "        \"\"\"\n",
    "        names = glob.glob(folderpath+ \"/JPEGImages/*.jpg\")\n",
    "        self.imagenames = [re.split(\"\\\\.\", re.split(\"\\\\/\", names[i])[-1])[0] for i in range(len(names))]\n",
    "        self.imagenames = list(set(self.imagenames) & set(files))\n",
    "        np.random.shuffle(self.imagenames)\n",
    "        if sample != -1:\n",
    "            self.imagenames = self.imagenames[:sample]\n",
    "        self.folderpath = folderpath\n",
    "        self.transform = transform\n",
    "        self.max_truth = max_truth\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imagenames)\n",
    "    \n",
    "    \n",
    "    def parse_xml(self, xml_file):\n",
    "        # actual parsing \n",
    "        in_file = open(xml_file)\n",
    "        tree = ET.parse(in_file)\n",
    "        root = tree.getroot()\n",
    "        imsize = root.find('size')\n",
    "        w = int(imsize.find('width').text)\n",
    "        h = int(imsize.find('height').text)\n",
    "        all_bboxes = list()\n",
    "\n",
    "        for obj in root.iter('object'):\n",
    "            current = list()\n",
    "            name = obj.find('name').text\n",
    "            \n",
    "            class_id = np.argwhere(classes==name)[0][0]\n",
    "            \n",
    "            xmlbox = obj.find('bndbox')\n",
    "            xn = int(float(xmlbox.find('xmin').text))\n",
    "            xx = int(float(xmlbox.find('xmax').text))\n",
    "            yn = int(float(xmlbox.find('ymin').text))\n",
    "            yx = int(float(xmlbox.find('ymax').text))\n",
    "            current = np.array([class_id,xn,yn,xx,yx])\n",
    "            all_bboxes += [current]\n",
    "        \n",
    "        in_file.close()\n",
    "        return np.array(all_bboxes)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imagename = self.imagenames[idx]\n",
    "        image_file = os.path.join(self.folderpath, \"JPEGImages\", imagename + \".jpg\")\n",
    "        annotation_file = os.path.join(self.folderpath, \"Annotations\", imagename + \".xml\")\n",
    "        \n",
    "        bboxes = self.parse_xml(annotation_file)\n",
    "        \n",
    "        img_array = np.asarray(Image.open(open(image_file)))\n",
    "        datum = {\"image\": img_array, \"bboxes\": bboxes}\n",
    "            \n",
    "        if self.transform:\n",
    "            datum = self.transform(datum)\n",
    "\n",
    "        bboxes = datum['bboxes'].numpy()\n",
    "        \n",
    "        n_true = len(bboxes)\n",
    "        if len(bboxes) > self.max_truth:\n",
    "            bboxes = bboxes[:self.max_truth]\n",
    "            n_true = self.max_truth\n",
    "        else:\n",
    "            zero_fill = self.max_truth - len(bboxes)\n",
    "            null_pad = -1 * (np.ones(5*zero_fill).reshape(zero_fill, 5))\n",
    "            if n_true == 0:\n",
    "                bboxes = null_pad\n",
    "            else:\n",
    "                bboxes = np.concatenate([bboxes, null_pad])\n",
    "        \n",
    "        datum['bboxes'] = torch.from_numpy(bboxes)\n",
    "        datum['n_true'] = n_true\n",
    "        return datum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 2 elements, new values have 1 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-447b610949e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvoc_train2012\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain2012\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval2012\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvoc_train2012\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'filepath'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvoc_train2012\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   3625\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3626\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3627\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3628\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3629\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m   3072\u001b[0m             raise ValueError('Length mismatch: Expected axis has %d elements, '\n\u001b[1;32m   3073\u001b[0m                              \u001b[0;34m'new values have %d elements'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3074\u001b[0;31m                              (old_len, new_len))\n\u001b[0m\u001b[1;32m   3075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 2 elements, new values have 1 elements"
     ]
    }
   ],
   "source": [
    "val2012 = pd.read_csv(\"VOCdevkit2/VOC2012/ImageSets/Main/val.txt\", \n",
    "                      sep='\\s+', header=-1)\n",
    "\n",
    "train2012 = pd.read_csv(\"VOCdevkit2/VOC2012/ImageSets/Main/train_train.txt\", \n",
    "                        sep='\\s+', header=-1)\n",
    "\n",
    "voc_train2012 = pd.concat([train2012, val2012]).drop_duplicates()\n",
    "\n",
    "voc_train2012.columns = ['filepath']\n",
    "voc_train2012[\"train\"] = 1\n",
    "\n",
    "voc_2012 = pd.DataFrame({\"filepath\":\n",
    "                         glob.glob(\"VOCdevkit2/VOC2012/JPEGImages/*.jpg\")})\n",
    "\n",
    "voc_2012[\"filepath\"] = voc_2012['filepath'].map(lambda x: re.split(\"\\\\.\", re.split(\"\\\\/\", x)[-1])[0])\n",
    "\n",
    "voc_2012 = pd.merge(voc_2012, voc_train2012, on=\"filepath\", how=\"left\")\n",
    "\n",
    "voc_2012.fillna(0, inplace=True)\n",
    "\n",
    "train_2007 = pd.read_csv(\"VOCdevkit/VOC2007/ImageSets/Main/train_test.txt\", sep='\\s+', header=-1, dtype='str')\n",
    "val_2007 = pd.read_csv(\"VOCdevkit/VOC2007/ImageSets/Main/train_test.txt\", sep='\\s+', header=-1, dtype='str')\n",
    "test_2007 = pd.read_csv(\"VOCdevkit/VOC2007/ImageSets/Main/test.txt\", sep='\\s+', header=-1, dtype='str')\n",
    "\n",
    "voc_2007 = pd.concat([train_2007, test_2007, val_2007])\n",
    "voc_2007.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(image_size=416, sample=-1, batch_size=64):\n",
    "    transform_fn = transforms.Compose([RandomCrop(), \n",
    "                                   RandomFlip(), \n",
    "                                   Rescale((image_size, image_size)), \n",
    "                                   TransformBoxCoords(), \n",
    "                                   Normalize(),\n",
    "                                   EliminateSmallBoxes(0.025),\n",
    "                                   ToTensor()])\n",
    "    \n",
    "    voc_train_12 = VOCDataset(\"/home/maverick/Desktop/datasets/VOCdevkit/VOCdevkit/VOC2012/\", sample=sample, transform=transform_fn,\n",
    "                        files=voc_2012[voc_2012[\"train\"]==1][\"filepath\"].values)\n",
    "    voc_test_12 = VOCDataset(\"/home/maverick/Desktop/datasets/VOCdevkit/VOCdevkit/VOC2012/\", sample=sample, transform=transform_fn,\n",
    "                            files=voc_2012[voc_2012[\"train\"]==0][\"filepath\"].values)\n",
    "    voc_train_07 = VOCDataset(\"/home/maverick/Desktop/datasets/VOCdevkit/VOCdevkit/VOC2007/\", sample=sample, transform=transform_fn,\n",
    "                            files=voc_2007[0].values)\n",
    "\n",
    "    # Dataloader\n",
    "    \n",
    "    train_loader = DataLoader(ConcatDataset([voc_train_07, voc_train_12]), \n",
    "                              batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(voc_test_12, batch_size=batch_size, num_workers=4)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class define for conv, maxpool and others\n",
    "class Yolov2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Yolov2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm6 = nn.BatchNorm2d(256)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm7 = nn.BatchNorm2d(128)\n",
    "        self.conv8 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm8 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm9 = nn.BatchNorm2d(512)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm10 = nn.BatchNorm2d(256)\n",
    "        self.conv11 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm11 = nn.BatchNorm2d(512)\n",
    "        self.conv12 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm12 = nn.BatchNorm2d(256)\n",
    "        self.conv13 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm13 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv14 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm14 = nn.BatchNorm2d(1024)\n",
    "        self.conv15 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm15 = nn.BatchNorm2d(512)\n",
    "        self.conv16 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm16 = nn.BatchNorm2d(1024)\n",
    "        self.conv17 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batchnorm17 = nn.BatchNorm2d(512)\n",
    "        self.conv18 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm18 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.conv19 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm19 = nn.BatchNorm2d(1024)\n",
    "        self.conv20 = nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm20 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels=3072, out_channels=1024, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batchnorm21 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels=1024, out_channels=125, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def reorg_layer(self, x):\n",
    "        stride = 2\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        new_ht = height/stride\n",
    "        new_wd = width/stride\n",
    "        new_channels = channels * stride * stride\n",
    "        \n",
    "        passthrough = x.permute(0, 2, 3, 1)\n",
    "        passthrough = passthrough.contiguous().view(-1, new_ht, stride, new_wd, stride, channels)\n",
    "        passthrough = passthrough.permute(0, 1, 3, 2, 4, 5)\n",
    "        passthrough = passthrough.contiguous().view(-1, new_ht, new_wd, new_channels)\n",
    "        passthrough = passthrough.permute(0, 3, 1, 2)\n",
    "        return passthrough\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(F.leaky_relu(self.batchnorm1(self.conv1(x)), negative_slope=0.1), 2, stride=2)\n",
    "        out = F.max_pool2d(F.leaky_relu(self.batchnorm2(self.conv2(out)), negative_slope=0.1), 2, stride=2)\n",
    "        \n",
    "        out = F.leaky_relu(self.batchnorm3(self.conv3(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm4(self.conv4(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm5(self.conv5(out)), negative_slope=0.1)\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "        \n",
    "        out = F.leaky_relu(self.batchnorm6(self.conv6(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm7(self.conv7(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm8(self.conv8(out)), negative_slope=0.1)\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "\n",
    "        out = F.leaky_relu(self.batchnorm9(self.conv9(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm10(self.conv10(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm11(self.conv11(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm12(self.conv12(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm13(self.conv13(out)), negative_slope=0.1)\n",
    "        passthrough = self.reorg_layer(out)\n",
    "        out = F.max_pool2d(out, 2, stride=2)\n",
    "\n",
    "        out = F.leaky_relu(self.batchnorm14(self.conv14(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm15(self.conv15(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm16(self.conv16(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm17(self.conv17(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm18(self.conv18(out)), negative_slope=0.1)\n",
    "\n",
    "        out = F.leaky_relu(self.batchnorm19(self.conv19(out)), negative_slope=0.1)\n",
    "        out = F.leaky_relu(self.batchnorm20(self.conv20(out)), negative_slope=0.1)\n",
    "        \n",
    "        out = torch.cat([passthrough, out], 1)\n",
    "        out = F.leaky_relu(self.batchnorm21(self.conv21(out)), negative_slope=0.1)\n",
    "        out = self.conv22(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_weights(model):\n",
    "    group_mapping = defaultdict(lambda: defaultdict())\n",
    "    cnt = 0\n",
    "    for child in model.children():\n",
    "        if type(child) == nn.Conv2d:\n",
    "            cnt += 1\n",
    "            if cnt > 18:\n",
    "                break\n",
    "            group_mapping[cnt]['conv'] = child\n",
    "            group_mapping[cnt]['bias'] = child\n",
    "        else:\n",
    "            group_mapping[cnt]['bias'] = child\n",
    "\n",
    "\n",
    "    f = open('../darknet19_448.conv.23', 'rb')\n",
    "#     f = open('../yolo.weights', 'rb')\n",
    "    major, minor, revision, seen = struct.unpack('4i', f.read(16))\n",
    "    for i in range(1, 19):\n",
    "\n",
    "        bias_var = group_mapping[i]['bias']\n",
    "        cnt = int(bias_var.bias.size()[0])\n",
    "        bias_var.bias.data = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
    "        bias_var.weight.data = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
    "        bias_var.running_mean = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
    "        bias_var.running_var = torch.from_numpy(np.array(struct.unpack('%df' % cnt, f.read(4*cnt)))).float()\n",
    "\n",
    "        for param in bias_var.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        conv_var = group_mapping[i]['conv']\n",
    "        c_out, c_in, f1, f2 = conv_var.weight.size()\n",
    "        cnt = int(c_out * c_in * f1 * f2)\n",
    "        p = struct.unpack('%df' % cnt, f.read(4*cnt))\n",
    "        conv_var.weight.data = torch.from_numpy(np.reshape(p, [c_out, c_in, f1, f2])).float()\n",
    "        for param in conv_var.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Yolov2()\n",
    "    model = load_pretrained_weights(model)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    return model\n",
    "\n",
    "    \n",
    "def model_freeze_upto(model, layer_x):\n",
    "    childs = list(model.children())\n",
    "    for i in range(len(childs)):\n",
    "        child = childs[i]\n",
    "        for param in child.parameters():\n",
    "            if i < layer_x:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_overlap_iou(bboxes1, bboxes2, is_anchor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        bboxes1: shape (total_bboxes1, 4)\n",
    "            with x1, y1, x2, y2 point order.\n",
    "        bboxes2: shape (total_bboxes2, 4)\n",
    "            with x1, y1, x2, y2 point order.\n",
    "        p1 *-----        \n",
    "           |     |\n",
    "           |_____* p2\n",
    "    Returns:\n",
    "        Tensor with shape (total_bboxes1, total_bboxes2)\n",
    "        with the IoU (intersection over union) of bboxes1[i] and bboxes2[j]\n",
    "        in [i, j].\n",
    "    \"\"\"\n",
    "#     import pdb; pdb.set_trace()\n",
    "    x1, y1, w1, h1 = bboxes1.chunk(4, dim=-1)\n",
    "    x2, y2, w2, h2 = bboxes2.chunk(4, dim=-1)\n",
    "    \n",
    "    x11 = x1 - 0.5*w1\n",
    "    y11 = y1 - 0.5*h1\n",
    "    x12 = x1 + 0.5*w1\n",
    "    y12 = y1 + 0.5*h1\n",
    "    x21 = x2 - 0.5*w2\n",
    "    y21 = y2 - 0.5*h2\n",
    "    x22 = x2 + 0.5*w2\n",
    "    y22 = y2 + 0.5*h2\n",
    "    \n",
    "#     x11 = torch.clamp(x11, min=0, max=1)\n",
    "#     y11 = torch.clamp(y11, min=0, max=1)\n",
    "#     x12 = torch.clamp(x12, min=0, max=1)\n",
    "#     y12 = torch.clamp(y12, min=0, max=1)\n",
    "#     x21 = torch.clamp(x21, min=0, max=1)\n",
    "#     y21 = torch.clamp(y21, min=0, max=1)\n",
    "#     x22 = torch.clamp(x22, min=0, max=1)\n",
    "#     y22 = torch.clamp(y22, min=0, max=1)\n",
    "    \n",
    "\n",
    "    xI1 = torch.max(x11, x21.transpose(1, 0))\n",
    "    yI1 = torch.max(y11, y21.transpose(1, 0))\n",
    "    \n",
    "    xI2 = torch.min(x12, x22.transpose(1, 0))\n",
    "    yI2 = torch.min(y12, y22.transpose(1, 0))\n",
    "\n",
    "    inner_box_w = torch.clamp((xI2 - xI1), min=0)\n",
    "    inner_box_h = torch.clamp((yI2 - yI1), min=0)\n",
    "    \n",
    "    inter_area = inner_box_w * inner_box_h\n",
    "    bboxes1_area = (x12 - x11) * (y12 - y11)\n",
    "    bboxes2_area = (x22 - x21) * (y22 - y21)\n",
    "\n",
    "    union = (bboxes1_area + bboxes2_area.transpose(1, 0)) - inter_area\n",
    "    return torch.clamp(inter_area / union, min=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def Yolov2Loss(output, labels, n_truths):\n",
    "    B = meta['anchors']\n",
    "    C = meta['classes']\n",
    "    batch_size = meta['batch_size']\n",
    "    threshold = meta['threshold']\n",
    "    anchor_bias = meta['anchor_bias']\n",
    "    scale_no_obj = meta['scale_no_obj']\n",
    "    scale_coords = meta['scale_coords']\n",
    "    scale_class = meta['scale_class']\n",
    "    scale_obj = meta['scale_obj']\n",
    "    \n",
    "    H = output.size(2)\n",
    "    W = output.size(3)\n",
    "    \n",
    "    wh = Variable(torch.from_numpy(np.reshape([W, H], [1, 1, 1, 1, 2]))).float()\n",
    "    anchor_bias_var = Variable(torch.from_numpy(np.reshape(anchor_bias, [1, 1, 1, B, 2]))).float()\n",
    "    \n",
    "    w_list = np.array(list(range(W)), np.float32)\n",
    "    wh_ids = Variable(torch.from_numpy(np.array(list(map(lambda x: np.array(list(itertools.product(w_list, [x]))), range(H)))).reshape(1, H, W, 1, 2))).float() \n",
    "    \n",
    "    zero_pad = Variable(torch.zeros(2).contiguous().view(1, 2)).float()\n",
    "    pad_var = Variable(torch.zeros(2*B).contiguous().view(B, 2)).float()\n",
    "    \n",
    "    loss = Variable(torch.Tensor([0])).float()\n",
    "    class_zeros = Variable(torch.zeros(C)).float()\n",
    "    mask_loss = Variable(torch.zeros(H*W*B*5).contiguous().view(H, W, B, 5)).float()\n",
    "    zero_coords_loss = Variable(torch.zeros(H*W*B*4).contiguous().view(H, W, B, 4)).float()\n",
    "    zero_coords_obj_loss = Variable(torch.zeros(H*W*B*5).contiguous().view(H, W, B, 5)).float()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        wh = wh.cuda()\n",
    "        wh_ids = wh_ids.cuda()\n",
    "        pad_var = pad_var.cuda()\n",
    "        zero_pad = zero_pad.cuda()\n",
    "        anchor_bias_var = anchor_bias_var.cuda()\n",
    "        \n",
    "        loss = loss.cuda()\n",
    "        mask_loss = mask_loss.cuda()\n",
    "        class_zeros = class_zeros.cuda()\n",
    "        zero_coords_loss = zero_coords_loss.cuda()\n",
    "        zero_coords_obj_loss = zero_coords_obj_loss.cuda()\n",
    "                           \n",
    "\n",
    "    anchor_bias_var = anchor_bias_var / wh\n",
    "    anchor_padded = torch.cat([pad_var, anchor_bias_var.contiguous().view(B, 2)], 1)\n",
    "\n",
    "    predicted = output.permute(0, 2, 3, 1)\n",
    "    predicted = predicted.contiguous().view(-1, H, W, B, (4 + 1 + C))\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    softmax = torch.nn.Softmax(dim=4)\n",
    "    \n",
    "    adjusted_xy = sigmoid(predicted[:, :, :, :, :2])\n",
    "    adjusted_obj = sigmoid(predicted[:, :, :, :, 4:5])\n",
    "    adjusted_classes = softmax(predicted[:, :, :, :, 5:])\n",
    "    \n",
    "    adjusted_coords = (adjusted_xy + wh_ids) / wh\n",
    "\n",
    "    adjusted_wh = torch.exp(predicted[:, :, :, :, 2:4]) * anchor_bias_var\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        \n",
    "        n_true = n_truths[batch]\n",
    "        if n_true == 0:\n",
    "            continue\n",
    "\n",
    "        pred_outputs = torch.cat([adjusted_coords[batch], adjusted_wh[batch]], 3)\n",
    "        true_labels = labels[batch, :n_true, 1:]\n",
    "        \n",
    "        bboxes_iou = bbox_overlap_iou(pred_outputs, true_labels, False)\n",
    "        \n",
    "        # objectness loss (if iou < threshold)\n",
    "        boxes_max_iou = torch.max(bboxes_iou, -1)[0]\n",
    "        all_obj_mask = boxes_max_iou.le(threshold)\n",
    "        all_obj_loss = all_obj_mask.unsqueeze(-1).float() *(scale_no_obj * (-1 * adjusted_obj[batch]))\n",
    "        \n",
    "        # each anchor box will learn its bias (if batch < 12800)\n",
    "        all_coords_loss = zero_coords_loss.clone()\n",
    "        if meta['iteration'] < 12800:\n",
    "            all_coords_loss = scale_coords * torch.cat([(0.5 - adjusted_xy[batch]), (0 - predicted[batch, :, :, :, 2:4])], -1)\n",
    "        \n",
    "        coord_obj_loss = torch.cat([all_coords_loss, all_obj_loss], -1)\n",
    "        \n",
    "        batch_mask = mask_loss.clone()\n",
    "        truth_coord_obj_loss = zero_coords_obj_loss.clone()\n",
    "        # for every true label and anchor bias\n",
    "        for truth_iter in torch.arange(n_true):\n",
    "            truth_iter = int(truth_iter)\n",
    "            truth_box = labels[batch, truth_iter]\n",
    "            anchor_select = bbox_overlap_iou(torch.cat([zero_pad.t(), truth_box[3:]], 0).t(), anchor_padded, True)\n",
    "            \n",
    "            # find the responsible anchor box\n",
    "            anchor_id = torch.max(anchor_select, 1)[1]\n",
    "            \n",
    "            truth_i = (truth_box[1] * W)\n",
    "            w_i = truth_i.int() \n",
    "            truth_x = truth_i - w_i.float()\n",
    "            truth_j = (truth_box[2] * H)\n",
    "            h_j = truth_j.int()\n",
    "            truth_y = truth_j - h_j.float()\n",
    "            truth_wh = (truth_box[3:] / anchor_bias_var.contiguous().view(B, 2).index_select(0, anchor_id.long())).log()\n",
    "            if (truth_wh[0] == Variable( - torch.cuda.FloatTensor([float('inf')]))).data[0] == 1:\n",
    "                import pdb; pdb.set_trace()\n",
    "    \n",
    "            truth_coords = torch.cat([truth_x.unsqueeze(0), truth_y.unsqueeze(0), truth_wh], 1)\n",
    "            \n",
    "            predicted_output = predicted[batch].index_select(0, h_j.long()).index_select(1, w_i.long()).index_select(2, anchor_id.long())[0][0][0]\n",
    "            # coords loss\n",
    "            pred_xy = adjusted_xy[batch].index_select(0, h_j.long()).index_select(1, w_i.long()).index_select(2, anchor_id.long())[0][0][0]\n",
    "            pred_wh = predicted_output[2:4]\n",
    "            pred_coords = torch.cat([pred_xy, pred_wh], 0)\n",
    "            coords_loss = scale_coords * (truth_coords - pred_coords.unsqueeze(0))\n",
    "    \n",
    "            # objectness loss\n",
    "        \n",
    "            # given the responsible box - find iou\n",
    "            iou = bboxes_iou.index_select(0, h_j.long()).index_select(1, w_i.long()).index_select(2, anchor_id.long())[0][0][0][truth_iter]\n",
    "            obj_loss = scale_obj * (iou - sigmoid(predicted_output[4]))\n",
    "            truth_co_obj = torch.cat([coords_loss, obj_loss.view(1, 1)], 1)\n",
    "\n",
    "            # class prob loss\n",
    "            class_vec = class_zeros.index_fill(0, truth_box[0].long(), 1)\n",
    "            class_loss = scale_class * (class_vec - torch.nn.Softmax(dim=0)(predicted_output[5:]))\n",
    "            \n",
    "            mask_ones = Variable(torch.ones(5)).float()\n",
    "            if torch.cuda.is_available():\n",
    "                mask_ones = mask_ones.cuda()\n",
    "            \n",
    "            batch_mask[h_j.long(), w_i.long(), anchor_id.long()] = mask_ones\n",
    "            truth_coord_obj_loss[h_j.long(), w_i.long(), anchor_id.long()] = truth_co_obj\n",
    "            \n",
    "            loss += class_loss.pow(2).sum()\n",
    "#         import pdb; pdb.set_trace()\n",
    "        batch_coord_obj_loss = batch_mask * truth_coord_obj_loss + (1 - batch_mask) * coord_obj_loss\n",
    "        \n",
    "        loss += batch_coord_obj_loss.pow(2).sum()\n",
    "        \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nms_boxes(output, obj_thresh, iou_thresh):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    N, C, H, W = output.size()\n",
    "    N, C, H, W = int(N), int(C), int(H), int(W)\n",
    "    B = meta['anchors']\n",
    "    anchor_bias = meta['anchor_bias']\n",
    "    n_classes = meta['classes']\n",
    "    \n",
    "    # -1 => unprocesse, 0 => suppressed, 1 => retained\n",
    "    box_tags = Variable(-1 * torch.ones(H*W*B)).float()\n",
    "    \n",
    "    wh = Variable(torch.from_numpy(np.reshape([W, H], [1, 1, 1, 1, 2]))).float()\n",
    "    anchor_bias_var = Variable(torch.from_numpy(np.reshape(anchor_bias, [1, 1, 1, B, 2]))).float()\n",
    "    \n",
    "    w_list = np.array(list(range(W)), np.float32)\n",
    "    wh_ids = Variable(torch.from_numpy(np.array(list(map(lambda x: np.array(list(itertools.product(w_list, [x]))), range(H)))).reshape(1, H, W, 1, 2))).float() \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        wh = wh.cuda()\n",
    "        wh_ids = wh_ids.cuda()\n",
    "        box_tags = box_tags.cuda()\n",
    "        anchor_bias_var = anchor_bias_var.cuda()                           \n",
    "\n",
    "    anchor_bias_var = anchor_bias_var / wh\n",
    "\n",
    "    predicted = output.permute(0, 2, 3, 1)\n",
    "    predicted = predicted.contiguous().view(N, H, W, B, -1)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    softmax = torch.nn.Softmax(dim=4)\n",
    "    \n",
    "    adjusted_xy = sigmoid(predicted[:, :, :, :, :2])\n",
    "    adjusted_obj = sigmoid(predicted[:, :, :, :, 4:5])\n",
    "    adjusted_classes = softmax(predicted[:, :, :, :, 5:])\n",
    "    \n",
    "    adjusted_coords = (adjusted_xy + wh_ids) / wh\n",
    "    adjusted_wh = torch.exp(predicted[:, :, :, :, 2:4]) * anchor_bias_var\n",
    "\n",
    "    batch_boxes = defaultdict()\n",
    "\n",
    "    for n in range(N):\n",
    "        \n",
    "        scores = (adjusted_obj[n] * adjusted_classes[n]).contiguous().view(H*W*B, -1)\n",
    "    \n",
    "        class_probs = adjusted_classes[n].contiguous().view(H*W*B, -1)\n",
    "        class_ids = torch.max(class_probs, 1)[1]\n",
    "            \n",
    "        pred_outputs = torch.cat([adjusted_coords[n], adjusted_wh[n]], 3)\n",
    "        pred_bboxes = pred_outputs.contiguous().view(H*W*B, 4)\n",
    "        ious = bbox_overlap_iou(pred_bboxes, pred_bboxes, True)\n",
    "        \n",
    "        confidences = adjusted_obj[n].contiguous().view(H*W*B)\n",
    "        # get all boxes with tag -1\n",
    "        final_boxes = Variable(torch.FloatTensor())\n",
    "        if torch.cuda.is_available():\n",
    "            final_boxes = final_boxes.cuda()\n",
    "   \n",
    "        for class_id in range(n_classes):\n",
    "            bboxes_state = ((class_ids==class_id).float() * (scores[:, class_id] > obj_thresh).float() * box_tags).long().float()\n",
    "        \n",
    "            while (torch.sum(bboxes_state==-1) > 0).data[0]:\n",
    "                max_conf, index = torch.max(scores[:, class_id] * (bboxes_state==-1).float(), 0)\n",
    "                bboxes_state = ((ious[index] < iou_thresh)[0].float() * bboxes_state).long().float()\n",
    "                bboxes_state[index] = 1\n",
    "\n",
    "                index_vals = torch.cat([pred_bboxes[index], confidences[index].view(1, 1), class_probs[index]], 1)\n",
    "                if len(final_boxes.size()) == 0:\n",
    "                    final_boxes = index_vals\n",
    "                else:\n",
    "                    final_boxes = torch.cat([final_boxes, index_vals], 0)\n",
    "        \n",
    "        batch_boxes[n] = final_boxes\n",
    "        \n",
    "    return batch_boxes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Max Suppression\n",
    "def get_nms_detections(output, obj_thresh, iou_thresh):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    N, C, H, W = output.size()\n",
    "    N, C, H, W = int(N), int(C), int(H), int(W)\n",
    "    B = meta['anchors']\n",
    "    anchor_bias = meta['anchor_bias']\n",
    "    \n",
    "    # -1 => unprocesse, 0 => suppressed, 1 => retained\n",
    "    box_tags = Variable(-1 * torch.ones(H*W*B)).float()\n",
    "    \n",
    "    wh = Variable(torch.from_numpy(np.reshape([W, H], [1, 1, 1, 1, 2]))).float()\n",
    "    anchor_bias_var = Variable(torch.from_numpy(np.reshape(anchor_bias, [1, 1, 1, B, 2]))).float()\n",
    "    \n",
    "    w_list = np.array(list(range(W)), np.float32)\n",
    "    wh_ids = Variable(torch.from_numpy(np.array(list(map(lambda x: np.array(list(itertools.product(w_list, [x]))), range(H)))).reshape(1, H, W, 1, 2))).float() \n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        wh = wh.cuda()\n",
    "        wh_ids = wh_ids.cuda()\n",
    "        box_tags = box_tags.cuda()\n",
    "        anchor_bias_var = anchor_bias_var.cuda()                           \n",
    "\n",
    "    anchor_bias_var = anchor_bias_var / wh\n",
    "\n",
    "    predicted = output.permute(0, 2, 3, 1)\n",
    "    predicted = predicted.contiguous().view(N, H, W, B, -1)\n",
    "    \n",
    "\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    softmax = torch.nn.Softmax(dim=4)\n",
    "    \n",
    "    adjusted_xy = sigmoid(predicted[:, :, :, :, :2])\n",
    "    adjusted_obj = sigmoid(predicted[:, :, :, :, 4:5])\n",
    "    adjusted_classes = softmax(predicted[:, :, :, :, 5:])\n",
    "    \n",
    "    adjusted_coords = (adjusted_xy + wh_ids) / wh\n",
    "    adjusted_wh = torch.exp(predicted[:, :, :, :, 2:4]) * anchor_bias_var\n",
    "\n",
    "    batch_boxes = defaultdict()\n",
    "\n",
    "    for n in range(N):\n",
    "        \n",
    "        class_probs = adjusted_classes[n].contiguous().view(H*W*B, -1)\n",
    "        pred_outputs = torch.cat([adjusted_coords[n], adjusted_wh[n]], 3)\n",
    "        pred_bboxes = pred_outputs.contiguous().view(H*W*B, 4)\n",
    "        ious = bbox_overlap_iou(pred_bboxes, pred_bboxes, True)\n",
    "        \n",
    "        confidences = adjusted_obj[n].contiguous().view(H*W*B)\n",
    "        bboxes_state = ((confidences>obj_thresh).float() * box_tags).long().float()\n",
    "        \n",
    "        # get all boxes with tag -1\n",
    "        final_boxes = Variable(torch.FloatTensor())\n",
    "        if torch.cuda.is_available():\n",
    "            final_boxes = final_boxes.cuda()\n",
    "        while (torch.sum(bboxes_state==-1) > 0).data[0]:\n",
    "            max_conf, index = torch.max(confidences * (bboxes_state==-1).float(), 0)\n",
    "            bboxes_state = ((ious[index] < iou_thresh)[0].float() * bboxes_state).long().float()\n",
    "            bboxes_state[index] = 1\n",
    "            \n",
    "            index_vals = torch.cat([pred_bboxes[index], confidences[index].view(1, 1), class_probs[index]], 1)\n",
    "            if len(final_boxes.size()) == 0:\n",
    "                final_boxes = index_vals\n",
    "            else:\n",
    "                final_boxes = torch.cat([final_boxes, index_vals], 0)\n",
    "        \n",
    "        batch_boxes[n] = final_boxes\n",
    "        \n",
    "    return batch_boxes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map(boxes_dict, iou_threshold=0.5):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    v = Variable(torch.zeros(1))\n",
    "    if torch.cuda.is_available():\n",
    "        v = v.cuda()\n",
    "    \n",
    "    if (len(boxes_dict['ground_truth'].size())==0) | (len(boxes_dict['prediction'].size())==0):\n",
    "        return v\n",
    "\n",
    "    gt = boxes_dict['ground_truth']\n",
    "    pr = boxes_dict['prediction']\n",
    "\n",
    "    gt_matched = Variable(-torch.ones(gt.size(0)))\n",
    "    pr_matched = Variable(-torch.ones(pr.size(0)))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gt_matched = gt_matched.cuda()\n",
    "        pr_matched = pr_matched.cuda()\n",
    "            \n",
    "    for i in range(len(pr)):\n",
    "        b = pr[i]\n",
    "        ious = bbox_overlap_iou(b[:4].view(1, 4), gt, True)\n",
    "        matched_scores = (gt_matched == -1).float() * (ious[0]>iou_threshold).float() * ious[0]\n",
    "        if torch.sum(matched_scores).data[0] > 0:\n",
    "            gt_idx = torch.max(matched_scores, 0)[1]\n",
    "            gt_matched[gt_idx] = i\n",
    "            pr_matched[i] = gt_idx\n",
    "        \n",
    "    tp = (pr_matched != -1).float()\n",
    "    fp = (pr_matched == -1).float()\n",
    "    tp_cumsum = torch.cumsum(tp, 0)\n",
    "    fp_cumsum = torch.cumsum(fp, 0)\n",
    "    n_corrects = tp_cumsum * tp\n",
    "    total = tp_cumsum + fp_cumsum\n",
    "    precision = n_corrects / total\n",
    "    for i in range(precision.size(0)):\n",
    "        precision[i] = torch.max(precision[i:])\n",
    "\n",
    "    average_precision = torch.sum(precision) / len(gt)\n",
    "    return average_precision\n",
    "    \n",
    "    \n",
    "def evaluation(ground_truths, nms_output, n_truths, iou_thresh):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    N = ground_truths.size(0)\n",
    "    \n",
    "    mean_avg_precision = Variable(torch.FloatTensor([0]))\n",
    "    if torch.cuda.is_available():\n",
    "        mean_avg_precision = mean_avg_precision.cuda()\n",
    "\n",
    "    for batch in range(int(N)):\n",
    "        category_map = defaultdict(lambda: defaultdict(lambda: torch.FloatTensor()))\n",
    "        \n",
    "        if n_truths[batch] == 0:\n",
    "            continue\n",
    "\n",
    "        ground_truth = ground_truths[batch, :n_truths[batch]]\n",
    "        for gt in ground_truth:\n",
    "            gt_class = gt[0].int().data[0]\n",
    "            t1 = category_map[gt_class]['ground_truth']\n",
    "            if len(t1.size()) == 0:\n",
    "                t1 = gt[1:].unsqueeze(0)\n",
    "            else:\n",
    "                t1 = torch.cat([t1, gt[1:].unsqueeze(0)], 0)\n",
    "            category_map[gt_class]['ground_truth'] = t1\n",
    "            \n",
    "        nms_boxes = nms_output[batch]\n",
    "        if len(nms_boxes.size()) == 0:\n",
    "            continue\n",
    "\n",
    "        for box in nms_boxes:\n",
    "            class_id = (torch.max(box[5:], 0)[1]).int().data[0]\n",
    "            t2 = category_map[class_id]['prediction']\n",
    "            if len(t2.size()) == 0:\n",
    "                t2 = box[:5].unsqueeze(0)\n",
    "            else:\n",
    "                t2 = torch.cat([t2, box[:5].unsqueeze(0)], 0)\n",
    "            category_map[class_id]['prediction'] = t2\n",
    "        cat_ids = category_map.keys()\n",
    "#         return category_map\n",
    "        mean_avg_precision += torch.mean(torch.cat([calc_map(category_map[cat_id], iou_thresh) for cat_id in cat_ids], 0 ))\n",
    "    return mean_avg_precision/N\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(output, labels, n_true, iou_thresh):\n",
    "    nms_output = get_nms_boxes(output, 0.24, 0.3)\n",
    "    mean_avg_prec = evaluation(labels, nms_output, n_true.numpy(), iou_thresh)\n",
    "    return mean_avg_prec\n",
    "\n",
    "def train(model, train_data, opt, iou_thresh):\n",
    "    train_images = Variable(train_data[\"image\"], requires_grad=True).cuda().float()\n",
    "    train_labels = Variable(train_data[\"bboxes\"], requires_grad=False).cuda().float()\n",
    "    train_n_true = train_data[\"n_true\"]\n",
    "    opt.zero_grad()\n",
    "    train_output = model(train_images)\n",
    "    loss = Yolov2Loss(train_output, train_labels, train_n_true.numpy())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    train_map = get_map(train_output, train_labels, train_n_true, iou_thresh)\n",
    "    return loss, train_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, test_data, iou_thresh):\n",
    "    test_images = Variable(test_data[\"image\"]).cuda().float()\n",
    "    test_labels = Variable(test_data[\"bboxes\"]).cuda().float()\n",
    "    test_n_true = test_data[\"n_true\"]\n",
    "\n",
    "    test_output = model(test_images)\n",
    "    test_loss = Yolov2Loss(test_output, test_labels, test_n_true.numpy())\n",
    "    test_map = get_map(test_output, test_labels, test_n_true, iou_thresh)\n",
    "    return test_loss, test_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boundary_box(image_dict):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    image = image_dict['image']\n",
    "    bboxes = image_dict['bboxes']\n",
    "    plt.imshow(image)\n",
    "    for bbox in bboxes:\n",
    "        xmin = bbox[1]\n",
    "        ymin = bbox[2]\n",
    "        xmax = bbox[3]\n",
    "        ymax = bbox[4]\n",
    "        plt.plot((xmin, xmin), (ymin, ymax), 'g')\n",
    "        plt.plot((xmin, xmax), (ymin, ymin), 'g')\n",
    "        plt.plot((xmax, xmax), (ymin, ymax), 'g')\n",
    "        plt.plot((xmin, xmax), (ymax, ymax), 'g')\n",
    "\n",
    "def draw_boundary_box_fraction(image_dict):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    image = image_dict['image']\n",
    "    height, width, channels = image.shape\n",
    "    bboxes = image_dict['bboxes']\n",
    "    plt.imshow(image)\n",
    "    for bbox in bboxes:\n",
    "        x = bbox[1] * width\n",
    "        y = bbox[2] * height\n",
    "        w = bbox[3] * width\n",
    "        h = bbox[4] * height\n",
    "        xmin = max(0, int(x - w*0.5))\n",
    "        ymin = max(0, int(y - h*0.5))\n",
    "        xmax = min(int(x + w*0.5), width)\n",
    "        ymax = min(int(y + h*0.5), height)\n",
    "        plt.plot((xmin, xmin), (ymin, ymax), 'g')\n",
    "        plt.plot((xmin, xmax), (ymin, ymin), 'g')\n",
    "        plt.plot((xmax, xmax), (ymin, ymax), 'g')\n",
    "        plt.plot((xmin, xmax), (ymax, ymax), 'g')\n",
    "\n",
    "def draw_bbox_torch(img_dict):\n",
    "    image = np.transpose(np.array(img_dict['image'].numpy()*255, np.uint8), (1, 2, 0))\n",
    "    bboxes = img_dict['bboxes'].numpy()\n",
    "    n_true = np.sum(bboxes[:, 0]!=-1)\n",
    "    bboxes = bboxes[:n_true]\n",
    "    draw_boundary_box_fraction({\"image\": image, 'bboxes': bboxes})\n",
    "\n",
    "def draw_bbox_nms(torch_img, nms_output):\n",
    "    image = np.transpose(np.array(torch_img.data.numpy()*255, np.uint8), (1, 2, 0))\n",
    "    bboxes = nms_output[:, :4].data.numpy()\n",
    "    bboxes = np.concatenate([np.zeros(len(bboxes)).reshape(len(bboxes), 1), bboxes], 1)\n",
    "    print (bboxes)\n",
    "    draw_boundary_box_fraction({\"image\": image, 'bboxes': bboxes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_var(layers):\n",
    "    params_grad = []\n",
    "    for layer in layers:\n",
    "        for param in layer.parameters():\n",
    "            if param.requires_grad:\n",
    "                params_grad.append(param)\n",
    "    return params_grad\n",
    "\n",
    "def get_optimizer(model, lrs, idxs=[]):\n",
    "    childs = list(model.children())\n",
    "    \n",
    "    if len(idxs)==0:\n",
    "        params = get_grad_var(childs)\n",
    "        opt = torch.optim.Adam(params, lr = lrs[0])\n",
    "        return opt\n",
    "\n",
    "    layer_groups = []\n",
    "    last_idx = 0\n",
    "    for idx in idxs:\n",
    "        layer_groups.append(childs[last_idx:idx])\n",
    "        last_idx = idx\n",
    "    \n",
    "    opt_params = zip(layer_groups, lrs)\n",
    "    opt = torch.optim.Adam([{'params': get_grad_var(p[0]), 'lr': p[1]} for p in opt_params])\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr():\n",
    "#     import pdb; pdb.set_trace()\n",
    "    train_loader, test_loader = get_data(batch_size=batch_size)\n",
    "    model = get_model()\n",
    "    start_lr = 0.00001\n",
    "    end_lr = 10\n",
    "    \n",
    "    opt = get_optimizer(model, [start_lr])\n",
    "    \n",
    "    sched = torch.optim.lr_scheduler.ExponentialLR(opt, ((end_lr/start_lr)** (1./meta['iterations_per_epoch'])))\n",
    "    \n",
    "    losses = []\n",
    "    lrs = []\n",
    "    best_loss = 10**10\n",
    "    meta['iteration'] = 0\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        if train_data[\"image\"].size(0) != batch_size:\n",
    "            break\n",
    "\n",
    "        sched.step()\n",
    "        loss, acc_map = train(model, train_data, opt, 0.3)\n",
    "        loss_val = loss.data[0]\n",
    "        \n",
    "        lrs.append(sched.get_lr()[0])\n",
    "        losses.append(loss_val)\n",
    "        if loss_val < best_loss:\n",
    "            best_loss = loss_val\n",
    "        \n",
    "        if loss_val > 4*best_loss:\n",
    "            break\n",
    "        \n",
    "        if meta['iteration']%20 == 0:\n",
    "            print (\"iteration {0} loss {1} map {2}\".format(meta['iteration'], loss_val, acc_map))\n",
    "        meta['iteration'] += batch_size\n",
    "        \n",
    "    return lrs, losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "meta = defaultdict()\n",
    "meta['anchors'] = 5\n",
    "meta['classes'] = 20\n",
    "meta['batch_size'] = batch_size\n",
    "meta['threshold'] = 0.6\n",
    "meta['anchor_bias'] = np.array([1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52])\n",
    "meta['scale_no_obj'] = 1\n",
    "meta['scale_coords'] = 1\n",
    "meta['scale_class'] = 1\n",
    "meta['scale_obj']  = 5\n",
    "meta['iteration'] = 0 \n",
    "meta['train_samples'] = len(voc_2007) + sum(voc_2012[\"train\"]==1)\n",
    "meta['iterations_per_epoch'] = meta['train_samples']/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs, losses, model = find_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrs[:250], losses[:250], 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingLR(_LRScheduler):\n",
    "\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        super(CosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "                (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n",
    "                for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layers(model, lr_est, n_cycles=1, epochs_per_cycle=1, cycle_mult=1):\n",
    "#     sample_size = 1920\n",
    "    train_loader, test_loader = get_data(batch_size=batch_size)\n",
    "\n",
    "    metrics = {'train':{'loss':[], 'acc': []}, 'test': {'loss': [], 'acc': []}}\n",
    "    # No Layer Groups\n",
    "    opt = get_optimizer(model, [lr_est])\n",
    "    \n",
    "    meta['iteration'] = 0\n",
    "    for cycle in range(n_cycles):\n",
    "                \n",
    "        # resetting the lr to original\n",
    "        param_groups = opt.param_groups\n",
    "        for param_group in param_groups:\n",
    "            param_group['lr'] = lr_est\n",
    "\n",
    "        cycle_len = meta['iterations_per_epoch'] * epochs_per_cycle\n",
    "        sched = CosineAnnealingLR(opt, cycle_len)\n",
    "\n",
    "        for epoch in range(epochs_per_cycle):\n",
    "            \n",
    "            for i, train_data in enumerate(train_loader):\n",
    "                if train_data[\"image\"].size(0) != batch_size:\n",
    "                    break\n",
    "                \n",
    "                sched.step()\n",
    "                loss, acc_map = train(model, train_data, opt, 0.3)\n",
    "                loss_val = loss.data[0]\n",
    "                acc_val = acc_map.data[0]\n",
    "                metrics['train']['loss'].append(loss_val)\n",
    "                metrics['train']['acc'].append(acc_val)\n",
    "                \n",
    "                meta['iteration'] += batch_size\n",
    "                \n",
    "                if meta['iteration']%(4*batch_size) == 0:\n",
    "                    total_test_loss = []\n",
    "                    total_test_acc = []\n",
    "                    for i, test_data in enumerate(test_loader):\n",
    "                        if test_data[\"image\"].size(0) != batch_size:\n",
    "                            break\n",
    "                        \n",
    "                        test_loss, test_acc = validate(model, test_data, 0.3)\n",
    "                        total_test_loss.append(test_loss.data[0])\n",
    "                        total_test_acc.append(test_acc.data[0])\n",
    "                    \n",
    "                    test_loss = np.mean(total_test_loss)\n",
    "                    test_acc = np.mean(total_test_acc)\n",
    "                    \n",
    "                    metrics['test']['loss'].append(test_loss)\n",
    "                    metrics['test']['acc'].append(test_acc)\n",
    "                    print (\"Iteration {0} Train Loss {1} Acc {2} Test Loss {3} Acc {4}\".format(\n",
    "                        meta['iteration'], loss_val, acc_val, test_loss, test_acc))\n",
    "                    \n",
    "                        \n",
    "        epochs_per_cycle *= cycle_mult\n",
    "    \n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = train_last_layers(model, start_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, open(\"model_2\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(open(\"model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_data(batch_size=batch_size)\n",
    "for i, test_data in enumerate(test_loader):\n",
    "    test_images = Variable(test_data[\"image\"], requires_grad=True).cuda().float()\n",
    "    test_labels = Variable(test_data[\"bboxes\"], requires_grad=False).cuda().float()\n",
    "    test_n_true = test_data[\"n_true\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnms = get_nms_boxes(test_output, 0.24, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bbox_torch({\"image\": test_images[0].cpu().data, \"bboxes\": test_labels[0].cpu().data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_bbox_nms(test_images[0].cpu(), nnms[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(nnms[0][0][5:], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[4].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model2.children())[4].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.children())[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_layers(model, lrs, layer_groups, n_cycles=1, epochs_per_cycle=1, cycle_mult=1):\n",
    "    \n",
    "    train_loader, test_loader = get_data()\n",
    "    opt = get_optimizer(model, [lr_est/100, lr_est/10, lr_est], [16, 36, 43])\n",
    "    metrics = {'train':{'loss':[], 'acc': []}, 'test': {'loss': [], 'acc': []}}\n",
    "    # No Layer Groups\n",
    "    opt = get_optimizer(model, [lr_est])\n",
    "    \n",
    "    meta['iteration'] = 0\n",
    "    for cycle in range(n_cycles):\n",
    "                \n",
    "        # resetting the lr to original\n",
    "        param_groups = opt.param_groups\n",
    "        for param_group in param_groups:\n",
    "            param_group['lr'] = lr_est\n",
    "\n",
    "        cycle_len = meta['iterations_per_epoch'] * epochs_per_cycle\n",
    "        sched = CosineAnnealingLR(opt, cycle_len)\n",
    "\n",
    "        for epoch in range(epochs_per_cycle):\n",
    "            \n",
    "            for i, train_data in enumerate(train_loader):\n",
    "                if train_data[\"image\"].size(0) != batch_size:\n",
    "                    break\n",
    "                \n",
    "                sched.step()\n",
    "                loss, acc_map = train(model, train_data, opt)\n",
    "                loss_val = loss.data[0]\n",
    "                acc_val = acc_map.data[0]\n",
    "                metrics['train']['loss'].append(loss_val)\n",
    "                metrics['train']['acc'].append(acc_val)\n",
    "                \n",
    "                meta['iteration'] += batch_size\n",
    "                \n",
    "                if meta['iteration']%100 == 0:\n",
    "                    total_test_loss = []\n",
    "                    total_test_acc = []\n",
    "                    for i, test_data in enumerate(test_loader):\n",
    "                        if test_data[\"image\"].size(0) != batch_size:\n",
    "                            break\n",
    "                        \n",
    "                        test_loss, test_acc = validate(model, test_data)\n",
    "                        total_test_loss.append(test_loss.data[0])\n",
    "                        total_test_acc.append(test_acc.data[0])\n",
    "                    \n",
    "                    test_loss = np.mean(total_test_loss)\n",
    "                    test_acc = np.mean(total_test_acc)\n",
    "                    \n",
    "                    metrics['test']['loss'].append(test_loss)\n",
    "                    metrics['test']['acc'].append(test_acc)\n",
    "                    print (\"Iteration {0} Train Loss {1} Acc {2} Test Loss {3} Acc {4}\".format(\n",
    "                        meta['iteration'], loss_val, acc_val, test_loss, test_acc))\n",
    "                    \n",
    "                        \n",
    "        epochs_per_cycle *= cycle_mult\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        if train_data[\"image\"].size(0) != batch_size:\n",
    "            continue\n",
    "        train_images = Variable(train_data[\"image\"], requires_grad=True).cuda().float()\n",
    "        train_labels = Variable(train_data[\"bboxes\"], requires_grad=False).cuda().float()\n",
    "        train_n_true = train_data[\"n_true\"]\n",
    "        \n",
    "        sched.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        updated_lr = current_lr * 1.0223509385032303\n",
    "        print (\"current lr -- updated lr\", current_lr, updated_lr)\n",
    "        optimizer.param_groups[0]['lr'] = updated_lr\n",
    "        train_output = model(train_images)\n",
    "        \n",
    "        try:\n",
    "            loss = Yolov2Loss(train_output, train_labels, train_n_true.numpy())\n",
    "        except Exception as e:\n",
    "            print (iteration, i)\n",
    "            raise e\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_vals.append(updated_lr)\n",
    "        loss_vals.append(loss.data[0])\n",
    "        \n",
    "        iteration += batch_size\n",
    "        \n",
    "        if iteration%2 == 0:\n",
    "            \n",
    "            test_total = 0\n",
    "            test_total_map = torch.cuda.FloatTensor([0])\n",
    "            for j, test_data in enumerate(test_loader):\n",
    "                test_images = Variable(test_data[\"image\"]).cuda().float()\n",
    "                test_labels = Variable(test_data[\"bboxes\"]).cuda().float()\n",
    "                test_n_true = test_data[\"n_true\"]\n",
    "                \n",
    "                test_output = model(test_images)\n",
    "                test_nms = get_nms_detections(test_output, 0.4, 0.4)\n",
    "                \n",
    "                test_map = evaluation(test_labels, test_nms, test_n_true.numpy())\n",
    "                test_total_map += test_map.data\n",
    "                test_total += 1\n",
    "            test_avg_map = test_total_map / test_total\n",
    "            \n",
    "            print (\"Iteration {} loss {} accuracy {}\".format(iteration, loss.data[0], test_avg_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, b in enumerate(train_loader):\n",
    "    img = b['image']\n",
    "    bboxes = b['bboxes']\n",
    "    break\n",
    "\n",
    "draw_bbox_torch({\"image\": img[0], \"bboxes\":bboxes[0]})\n",
    "\n",
    "out = model(Variable(img.float().cuda()))\n",
    "\n",
    "iteration = 1\n",
    "loss = Yolov2Loss(out, Variable(bboxes).cuda().float(), b['n_true'].numpy())\n",
    "\n",
    "t = get_nms_detections(out)\n",
    "\n",
    "mean_ap = evaluation(Variable(bboxes).float().cuda(), t, b['n_true'].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
